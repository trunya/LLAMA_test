{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqsfsxuQhLig"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meta-llama/llama-recipes/blob/main/recipes/quickstart/Prompt_Engineering_with_Llama_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Prompt Engineering with Llama 3.1\n",
        "\n",
        "Prompt engineering is using natural language to produce a desired response from a large language model (LLM).\n",
        "\n",
        "This interactive guide covers prompt engineering & best practices with Llama 3.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28_CltXxhLij"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQFp9ZvghLij"
      },
      "source": [
        "### Why now?\n",
        "\n",
        "[Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) introduced the world to transformer neural networks (originally for machine translation). Transformers ushered an era of generative AI with diffusion models for image creation and large language models (`LLMs`) as **programmable deep learning networks**.\n",
        "\n",
        "Programming foundational LLMs is done with natural language – it doesn't require training/tuning like ML models of the past. This has opened the door to a massive amount of innovation and a paradigm shift in how technology can be deployed. The science/art of using natural language to program language models to accomplish a task is referred to as **Prompt Engineering**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT4awAWuhLik"
      },
      "source": [
        "### Llama Models\n",
        "\n",
        "In 2023, Meta introduced the [Llama language models](https://ai.meta.com/llama/) (Llama Chat, Code Llama, Llama Guard). These are general purpose, state-of-the-art LLMs.\n",
        "\n",
        "Llama models come in varying parameter sizes. The smaller models are cheaper to deploy and run; the larger models are more capable.\n",
        "\n",
        "#### Llama 3.1\n",
        "1. `llama-3.1-8b` - base pretrained 8 billion parameter model\n",
        "1. `llama-3.1-70b` - base pretrained 70 billion parameter model\n",
        "1. `llama-3.1-405b` - base pretrained 405 billion parameter model\n",
        "1. `llama-3.1-8b-instruct` - instruction fine-tuned 8 billion parameter model\n",
        "1. `llama-3.1-70b-instruct` - instruction fine-tuned 70 billion parameter model\n",
        "1. `llama-3.1-405b-instruct` - instruction fine-tuned 405 billion parameter model (flagship)\n",
        "\n",
        "\n",
        "#### Llama 3\n",
        "1. `llama-3-8b` - base pretrained 8 billion parameter model\n",
        "1. `llama-3-70b` - base pretrained 70 billion parameter model\n",
        "1. `llama-3-8b-instruct` - instruction fine-tuned 8 billion parameter model\n",
        "1. `llama-3-70b-instruct` - instruction fine-tuned 70 billion parameter model (flagship)\n",
        "\n",
        "#### Llama 2\n",
        "1. `llama-2-7b` - base pretrained 7 billion parameter model\n",
        "1. `llama-2-13b` - base pretrained 13 billion parameter model\n",
        "1. `llama-2-70b` - base pretrained 70 billion parameter model\n",
        "1. `llama-2-7b-chat` - chat fine-tuned 7 billion parameter model\n",
        "1. `llama-2-13b-chat` - chat fine-tuned 13 billion parameter model\n",
        "1. `llama-2-70b-chat` - chat fine-tuned 70 billion parameter model (flagship)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baBjcQLZhLil"
      },
      "source": [
        "Code Llama is a code-focused LLM built on top of Llama 2 also available in various sizes and finetunes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "650WqeKWhLil"
      },
      "source": [
        "#### Code Llama\n",
        "1. `codellama-7b` - code fine-tuned 7 billion parameter model\n",
        "1. `codellama-13b` - code fine-tuned 13 billion parameter model\n",
        "1. `codellama-34b` - code fine-tuned 34 billion parameter model\n",
        "1. `codellama-70b` - code fine-tuned 70 billion parameter model\n",
        "1. `codellama-7b-instruct` - code & instruct fine-tuned 7 billion parameter model\n",
        "2. `codellama-13b-instruct` - code & instruct fine-tuned 13 billion parameter model\n",
        "3. `codellama-34b-instruct` - code & instruct fine-tuned 34 billion parameter model\n",
        "3. `codellama-70b-instruct` - code & instruct fine-tuned 70 billion parameter model\n",
        "1. `codellama-7b-python` - Python fine-tuned 7 billion parameter model\n",
        "2. `codellama-13b-python` - Python fine-tuned 13 billion parameter model\n",
        "3. `codellama-34b-python` - Python fine-tuned 34 billion parameter model\n",
        "3. `codellama-70b-python` - Python fine-tuned 70 billion parameter model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFmYW0CthLil"
      },
      "source": [
        "## Getting an LLM\n",
        "\n",
        "Large language models are deployed and accessed in a variety of ways, including:\n",
        "\n",
        "1. **Self-hosting**: Using local hardware to run inference. Ex. running Llama on your Macbook Pro using [llama.cpp](https://github.com/ggerganov/llama.cpp).\n",
        "    * Best for privacy/security or if you already have a GPU.\n",
        "1. **Cloud hosting**: Using a cloud provider to deploy an instance that hosts a specific model. Ex. running Llama on cloud providers like AWS, Azure, GCP, and others.\n",
        "    * Best for customizing models and their runtime (ex. fine-tuning a model for your use case).\n",
        "1. **Hosted API**: Call LLMs directly via an API. There are many companies that provide Llama inference APIs including AWS Bedrock, Replicate, Anyscale, Together and others.\n",
        "    * Easiest option overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaA4Hlf-hLim"
      },
      "source": [
        "### Hosted APIs\n",
        "\n",
        "Hosted APIs are the easiest way to get started. We'll use them here. There are usually two main endpoints:\n",
        "\n",
        "1. **`completion`**: generate a response to a given prompt (a string).\n",
        "1. **`chat_completion`**: generate the next message in a list of messages, enabling more explicit instruction and context for use cases like chatbots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W63e7juchLim"
      },
      "source": [
        "## Tokens\n",
        "\n",
        "LLMs process inputs and outputs in chunks called *tokens*. Think of these, roughly, as words – each model will have its own tokenization scheme. For example, this sentence...\n",
        "\n",
        "> Our destiny is written in the stars.\n",
        "\n",
        "...is tokenized into `[\"Our\", \" destiny\", \" is\", \" written\", \" in\", \" the\", \" stars\", \".\"]` for Llama 3. See [this](https://tiktokenizer.vercel.app/?model=meta-llama%2FMeta-Llama-3-8B) for an interactive tokenizer tool.\n",
        "\n",
        "Tokens matter most when you consider API pricing and internal behavior (ex. hyperparameters).\n",
        "\n",
        "Each model has a maximum context length that your prompt cannot exceed. That's 128k tokens for Llama 3.1, 4K for Llama 2, and 100K for Code Llama.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP0GGsRbhLim"
      },
      "source": [
        "## Notebook Setup\n",
        "\n",
        "The following APIs will be used to call LLMs throughout the guide. As an example, we'll call Llama 3.1 chat using [Grok](https://console.groq.com/playground?model=llama3-70b-8192).\n",
        "\n",
        "To install prerequisites run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-ggN9I0hLin",
        "outputId": "60b0d9b3-1d06-444c-e724-912489f32daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n",
            "Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "a=userdata.get('groq1')"
      ],
      "metadata": {
        "id": "EG1emk-7i_D-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNMHJYyVjFQf",
        "outputId": "49184fb3-385c-4098-96d8-518f7daf70b1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4JAUeu2-hLio"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict, List\n",
        "from groq import Groq\n",
        "\n",
        "# Get a free API key from https://console.groq.com/keys\n",
        "os.environ[\"GROQ_API_KEY\"] = a\n",
        "\n",
        "\n",
        "#LLAMA3_405B_INSTRUCT = \"llama-3.1-405b-reasoning\" # Note: Groq currently only gives access here to paying customers for 405B model\n",
        "LLAMA3_70B_INSTRUCT = \"llama-3.1-70b-versatile\"\n",
        "LLAMA3_8B_INSTRUCT = \"llama3.1-8b-instant\"\n",
        "\n",
        "DEFAULT_MODEL = LLAMA3_70B_INSTRUCT\n",
        "\n",
        "client = Groq()\n",
        "\n",
        "def assistant(content: str):\n",
        "    return { \"role\": \"assistant\", \"content\": content }\n",
        "\n",
        "def user(content: str):\n",
        "    return { \"role\": \"user\", \"content\": content }\n",
        "\n",
        "def chat_completion(\n",
        "    messages: List[Dict],\n",
        "    model = DEFAULT_MODEL,\n",
        "    temperature: float = 0.6,\n",
        "    top_p: float = 0.9,\n",
        ") -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def completion(\n",
        "    prompt: str,\n",
        "    model: str = DEFAULT_MODEL,\n",
        "    temperature: float = 0.6,\n",
        "    top_p: float = 0.9,\n",
        ") -> str:\n",
        "    return chat_completion(\n",
        "        [user(prompt)],\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "\n",
        "def complete_and_print(prompt: str, model: str = DEFAULT_MODEL):\n",
        "    print(f'==============\\n{prompt}\\n==============')\n",
        "    response = completion(prompt, model)\n",
        "    print(response, end='\\n\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN-V72VihLip"
      },
      "source": [
        "### Completion APIs\n",
        "\n",
        "Let's try Llama 3.1!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhP_r3uGhLip",
        "outputId": "28ef913b-c4fc-4f05-c62e-216a044e2aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============\n",
            "The typical color of the sky is: \n",
            "==============\n",
            "Blue.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "complete_and_print(\"The typical color of the sky is: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhuWgH9JhLip",
        "outputId": "a268d94f-4d24-4aaa-cdd0-5a70698c1b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============\n",
            "which model version are you?\n",
            "==============\n",
            "I'm a large language model, my model version is InstructLLaMA, and my knowledge cutoff is currently December 2023, but I don't have a specific version number.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "complete_and_print(\"which model version are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rejZUpxohLiq",
        "outputId": "ac33f92b-40f4-4f3d-f680-009e0353aa96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[temperature: 0.01 | top_p: 0.01]\n",
            "Softly gentle eyes\n",
            "Llama's gentle, fuzzy form\n",
            "Misty mountain home\n",
            "\n",
            "[temperature: 0.01 | top_p: 0.01]\n",
            "Softly gentle eyes\n",
            "Llama's gentle, fuzzy form\n",
            "Misty mountain home\n",
            "\n",
            "[temperature: 1.0 | top_p: 1.0]\n",
            "Soft, woolly creature\n",
            "Ears perked up with gentle gaze\n",
            "Llama's gentle soul\n",
            "\n",
            "[temperature: 1.0 | top_p: 1.0]\n",
            "Fuzzy, gentle eyes\n",
            "Softly pads across the land\n",
            "Majestic delight\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def print_tuned_completion(temperature: float, top_p: float):\n",
        "    response = completion(\"Write a haiku about llamas\", temperature=temperature, top_p=top_p)\n",
        "    print(f'[temperature: {temperature} | top_p: {top_p}]\\n{response.strip()}\\n')\n",
        "\n",
        "print_tuned_completion(0.01, 0.01)\n",
        "print_tuned_completion(0.01, 0.01)\n",
        "# These two generations are highly likely to be the same\n",
        "\n",
        "print_tuned_completion(1.0, 1.0)\n",
        "print_tuned_completion(1.0, 1.0)\n",
        "# These two generations are highly likely to be different"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8yCHANghLiq"
      },
      "source": [
        "## Prompting Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K44zpcm3hLiq"
      },
      "source": [
        "### Explicit Instructions\n",
        "\n",
        "Detailed, explicit instructions produce better results than open-ended prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WyMmCePhLiq"
      },
      "outputs": [],
      "source": [
        "complete_and_print(prompt=\"Describe quantum physics in one short sentence of no more than 12 words\")\n",
        "# Returns a succinct explanation of quantum physics that mentions particles and states existing simultaneously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcSACCFAhLiq"
      },
      "source": [
        "You can think about giving explicit instructions as using rules and restrictions to how Llama 3 responds to your prompt.\n",
        "\n",
        "- Stylization\n",
        "    - `Explain this to me like a topic on a children's educational network show teaching elementary students.`\n",
        "    - `I'm a software engineer using large language models for summarization. Summarize the following text in under 250 words:`\n",
        "    - `Give your answer like an old timey private investigator hunting down a case step by step.`\n",
        "- Formatting\n",
        "    - `Use bullet points.`\n",
        "    - `Return as a JSON object.`\n",
        "    - `Use less technical terms and help me apply it in my work in communications.`\n",
        "- Restrictions\n",
        "    - `Only use academic papers.`\n",
        "    - `Never give sources older than 2020.`\n",
        "    - `If you don't know the answer, say that you don't know.`\n",
        "\n",
        "Here's an example of giving explicit instructions to give more specific results by limiting the responses to recently created sources."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: accept a csv file and ask llama to explain ti\n",
        "\n",
        "import sys\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from typing import Dict, List\n",
        "from groq import Groq\n",
        "import csv\n",
        "\n",
        "# ... (rest of your existing code)\n",
        "\n",
        "def analyze_csv_with_llama(csv_file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file and asks Llama to explain its contents.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(csv_file_path, 'r') as file:\n",
        "            reader = csv.reader(file)\n",
        "            header = next(reader, None)  # Get the header row\n",
        "            data = list(reader)  # Read the remaining data rows\n",
        "\n",
        "            if header is None:\n",
        "                prompt = f\"Can you give me global interpretation for the 2 assessments for the athlete\"\n",
        "            else:\n",
        "                prompt = f\"Can you give me global interpretation for the 2 assessments for this specific athlete. The header is {header}. The data is:\\n{data}\"\n",
        "\n",
        "            explanation = completion(prompt)  # Get explanation from Llama\n",
        "            print(explanation)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: CSV file not found at {csv_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage (replace 'your_file.csv' with the actual path to your CSV)\n",
        "analyze_csv_with_llama('/content/FMS&MKS.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrhGSE6ekgL7",
        "outputId": "0aec703a-3ebe-4064-ea78-ff9c148ac1e6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided data, here's a global interpretation of the two assessments for the specific athlete:\n",
            "\n",
            "**Musculoskeletal Assessment:**\n",
            "\n",
            "The musculoskeletal assessment reveals several key findings that suggest the athlete has a complex pattern of movement and postural dysfunctions. The main findings include:\n",
            "\n",
            "1. **Right-sided dominance**: The athlete presents with a right-sided dominance, characterized by a higher right hip, increased weight-bearing on the right side, and a convex spine towards the right.\n",
            "2. **Upper quarter restrictions**: The athlete has restricted shoulder internal rotation on the right side, tender upper trapezius, and rhomboids and lats insertion. This suggests a potential issue with scapular stability and mobility.\n",
            "3. **Lower quarter restrictions**: The athlete has restricted bilateral hip external rotation, tight right glute medius, and tight left psoas. This suggests a potential issue with hip mobility and stability.\n",
            "4. **Postural dysfunctions**: The athlete presents with a convex spine towards the right, a higher right hip, and a lower right shoulder. This suggests a potential issue with core stability and posture.\n",
            "\n",
            "**Functional Movement Screening:**\n",
            "\n",
            "The functional movement screening reveals several key findings that suggest the athlete has a high risk of injury and movement dysfunction. The main findings include:\n",
            "\n",
            "1. **Poor squat mechanics**: The athlete demonstrates poor squat mechanics, characterized by a right hip hike, toes turning out, and lifting off. This suggests a potential issue with hip stability and mobility.\n",
            "2. **Hip hike and drop**: The athlete demonstrates a bilateral hip hike and drop during the hurdle step test, suggesting a potential issue with hip stability and mobility.\n",
            "3. **Shoulder mobility restrictions**: The athlete demonstrates restricted left external rotation and right internal rotation, suggesting a potential issue with shoulder mobility and stability.\n",
            "4. **Trunk stability issues**: The athlete demonstrates a hip lag during the trunk stability push-up test, suggesting a potential issue with core stability.\n",
            "\n",
            "**Global Interpretation:**\n",
            "\n",
            "Based on the findings from both assessments, it appears that the athlete has a complex pattern of movement and postural dysfunctions that increase their risk of injury. The athlete's right-sided dominance, upper quarter restrictions, and lower quarter restrictions suggest a potential issue with scapular stability, hip mobility, and core stability. The functional movement screening findings suggest that the athlete has poor squat mechanics, hip hike and drop, shoulder mobility restrictions, and trunk stability issues.\n",
            "\n",
            "**Recommendations:**\n",
            "\n",
            "Based on the findings, the following recommendations can be made:\n",
            "\n",
            "1. **Scapular stabilization exercises**: The athlete should perform exercises that target scapular stability, such as scapular push-ups and rows.\n",
            "2. **Hip mobility exercises**: The athlete should perform exercises that target hip mobility, such as lunges and leg swings.\n",
            "3. **Core stability exercises**: The athlete should perform exercises that target core stability, such as planks and side planks.\n",
            "4. **Shoulder mobility exercises**: The athlete should perform exercises that target shoulder mobility, such as shoulder rotations and scapular wall slides.\n",
            "5. **Functional movement training**: The athlete should participate in functional movement training that targets squat mechanics, hip stability, and trunk stability.\n",
            "\n",
            "By addressing these movement and postural dysfunctions, the athlete can reduce their risk of injury and improve their overall movement quality.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vwIzwE1qkuMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: accept an excel file and ask llama to explain it\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Assuming the uploaded file is named 'your_excel_file.xlsx'\n",
        "# Replace 'your_excel_file.xlsx' with the actual filename if different\n",
        "excel_filename = list(uploaded.keys())[0]\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel(excel_filename)\n",
        "    excel_data_string = df.to_string()  # Convert the DataFrame to a string\n",
        "\n",
        "    # Now, use the Llama model (assuming the 'completion' function from your code is available)\n",
        "    prompt = f\"Explain the following Excel data:\\n\\n{excel_data_string}\"\n",
        "    explanation = completion(prompt)\n",
        "    print(explanation)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "suMVyXuDkFgl",
        "outputId": "5930abae-872a-48e0-9437-b942bb591f7c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-97f8cb5a-a3db-43d6-a03b-f509a50a7bfb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-97f8cb5a-a3db-43d6-a03b-f509a50a7bfb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-5b1791a20d27>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Assuming the uploaded file is named 'your_excel_file.xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Replace 'your_excel_file.xlsx' with the actual filename if different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mexcel_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08tRdm4VhLiz"
      },
      "source": [
        "## Additional References\n",
        "- [PromptingGuide.ai](https://www.promptingguide.ai/)\n",
        "- [LearnPrompting.org](https://learnprompting.org/)\n",
        "- [Lil'Log Prompt Engineering Guide](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw4gBGr9hLiz"
      },
      "source": [
        "## Author & Contact\n",
        "\n",
        "Edited by [Dalton Flanagan](https://www.linkedin.com/in/daltonflanagan/) (dalton@meta.com) with contributions from Mohsen Agsen, Bryce Bortree, Ricardo Juan Palma Duran, Kaolin Fire, Thomas Scialom."
      ]
    }
  ],
  "metadata": {
    "captumWidgetMessage": [],
    "dataExplorerConfig": [],
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "last_base_url": "https://bento.edge.x2p.facebook.net/",
    "last_kernel_id": "161e2a7b-2d2b-4995-87f3-d1539860ecac",
    "last_msg_id": "4eab1242-d815b886ebe4f5b1966da982_543",
    "last_server_session_id": "4a7b41c5-ed66-4dcb-a376-22673aebb469",
    "operator_data": [],
    "outputWidgetContext": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}